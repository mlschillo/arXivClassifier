{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surprising-deviation",
   "metadata": {},
   "source": [
    "# Dear Theoretical Physics, if we pass eachother on the street in 10 years, will we even recognize eachother?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-infrastructure",
   "metadata": {},
   "source": [
    "As I bid a fond farewell to academia, and an enthusiastic 'Hello' to *industry*, I face the same questions regarding my core identity that so many have faced before me: 'Will people still think I'm smart?' 'What will I do? will it be fascinating, challenging, and rewarding?' 'Will I still be a physicist?' In this project, I would like to focus on the question of my continuation to *be* a physicist.  **The idea, as the title suggests, is to ask whether 10 years of progress will render theoretical physics unrecognizable.**  I am currently an expert in theoretical cosmology and string phenomenology, and very well versed in high energy theoretical phsyics in general, but for how long?\n",
    "\n",
    "To answer this question, I will train a new expert, not just in high energy theory, but in all subfields of physics. At least, I will train an expert AI to recognize and classify the abstract of a physics paper into its appropriate subfield. Then, I will train this AI specifically using physics papers published to arxiv.org in, say, 2008, and see how well this AI performs on classifying papers published in 2018.\n",
    "\n",
    "Other than answering questions pertaining to my personal identity crisis, this type of analysis might be of interest to the history of physics in quantifiably identifying paradigm shifts -- pivotal points where the face of a field shifts so as to render it unrecognizable to expers of the past."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-thompson",
   "metadata": {},
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-liabilities",
   "metadata": {},
   "source": [
    "I used the pretrained bert-base-cased model provided by Google and the Transformers library provided by Hugging Face. The architecture as well as intuitive descriptions of how all the pieces work were provided by THIS TUTORIAL. I trained the models on Google Colab's GPUs.  So overall, all of the heavy lifting was done for me. Thanks!\n",
    "\n",
    "All I had to do was clean and separate the data into the various training sets, fine tune the various models -- choosing hyperparameters such as learning rate, batch size, and training epochs, and collect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-central",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
